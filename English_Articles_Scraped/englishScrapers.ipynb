{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping\n",
    "- Scraping articles from english websites of popular News Websites Geo News and Samaa TV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Geo News: <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping articles for category 'sports'...\n",
      "\t--> Scraped article 1 in category 'sports'.\n",
      "\t--> Scraped article 2 in category 'sports'.\n",
      "\t--> Scraped article 3 in category 'sports'.\n",
      "\t--> Scraped article 4 in category 'sports'.\n",
      "\t--> Scraped article 5 in category 'sports'.\n",
      "\t--> Scraped article 6 in category 'sports'.\n",
      "\t--> Scraped article 7 in category 'sports'.\n",
      "\t--> Scraped article 8 in category 'sports'.\n",
      "\t--> Scraped article 9 in category 'sports'.\n",
      "\t--> Scraped article 10 in category 'sports'.\n",
      "\t--> Scraped article 11 in category 'sports'.\n",
      "\t--> Scraped article 12 in category 'sports'.\n",
      "\t--> Scraped article 13 in category 'sports'.\n",
      "\t--> Scraped article 14 in category 'sports'.\n",
      "\t--> Scraped article 15 in category 'sports'.\n",
      "\t--> Scraped article 16 in category 'sports'.\n",
      "\t--> Scraped article 17 in category 'sports'.\n",
      "\t--> Scraped article 18 in category 'sports'.\n",
      "\t--> Scraped article 19 in category 'sports'.\n",
      "\t--> Scraped article 20 in category 'sports'.\n",
      "\t--> Scraped article 21 in category 'sports'.\n",
      "\t--> Scraped article 22 in category 'sports'.\n",
      "\t--> Scraped article 23 in category 'sports'.\n",
      "\t--> Scraped article 24 in category 'sports'.\n",
      "\t--> Scraped article 25 in category 'sports'.\n",
      "\t--> Scraped article 26 in category 'sports'.\n",
      "\t--> Scraped article 27 in category 'sports'.\n",
      "\t--> Scraped article 28 in category 'sports'.\n",
      "\t--> Scraped article 29 in category 'sports'.\n",
      "\t--> Scraped article 30 in category 'sports'.\n",
      "\t--> Scraped article 31 in category 'sports'.\n",
      "\t--> Scraped article 32 in category 'sports'.\n",
      "\t--> Scraped article 33 in category 'sports'.\n",
      "\t--> Scraped article 34 in category 'sports'.\n",
      "\t--> Scraped article 35 in category 'sports'.\n",
      "\t--> Scraped article 36 in category 'sports'.\n",
      "\t--> Scraped article 37 in category 'sports'.\n",
      "\t--> Scraped article 38 in category 'sports'.\n",
      "\t--> Scraped article 39 in category 'sports'.\n",
      "\t--> Scraped article 40 in category 'sports'.\n",
      "\t--> Scraped article 41 in category 'sports'.\n",
      "\t--> Scraped article 42 in category 'sports'.\n",
      "\t--> Scraped article 43 in category 'sports'.\n",
      "\t--> Scraped article 44 in category 'sports'.\n",
      "\t--> Scraped article 45 in category 'sports'.\n",
      "\t--> Scraped article 46 in category 'sports'.\n",
      "\t--> Scraped article 47 in category 'sports'.\n",
      "\t--> Scraped article 48 in category 'sports'.\n",
      "\t--> Scraped article 49 in category 'sports'.\n",
      "\t--> Scraped article 50 in category 'sports'.\n",
      "\t--> Scraped article 51 in category 'sports'.\n",
      "\t--> Scraped article 52 in category 'sports'.\n",
      "\t--> Scraped article 53 in category 'sports'.\n",
      "\t--> Scraped article 54 in category 'sports'.\n",
      "\t--> Scraped article 55 in category 'sports'.\n",
      "\t--> Scraped article 56 in category 'sports'.\n",
      "\t--> Scraped article 57 in category 'sports'.\n",
      "\t--> Scraped article 58 in category 'sports'.\n",
      "Completed scraping 58 articles from category 'sports'.\n",
      "Scraping articles for category 'science'...\n",
      "\t--> Scraped article 1 in category 'science'.\n",
      "\t--> Scraped article 2 in category 'science'.\n",
      "\t--> Scraped article 3 in category 'science'.\n",
      "\t--> Scraped article 4 in category 'science'.\n",
      "\t--> Scraped article 5 in category 'science'.\n",
      "\t--> Scraped article 6 in category 'science'.\n",
      "\t--> Scraped article 7 in category 'science'.\n",
      "\t--> Scraped article 8 in category 'science'.\n",
      "\t--> Scraped article 9 in category 'science'.\n",
      "\t--> Scraped article 10 in category 'science'.\n",
      "\t--> Scraped article 11 in category 'science'.\n",
      "\t--> Scraped article 12 in category 'science'.\n",
      "\t--> Scraped article 13 in category 'science'.\n",
      "\t--> Scraped article 14 in category 'science'.\n",
      "\t--> Scraped article 15 in category 'science'.\n",
      "\t--> Scraped article 16 in category 'science'.\n",
      "\t--> Scraped article 17 in category 'science'.\n",
      "\t--> Scraped article 18 in category 'science'.\n",
      "\t--> Scraped article 19 in category 'science'.\n",
      "\t--> Scraped article 20 in category 'science'.\n",
      "\t--> Scraped article 21 in category 'science'.\n",
      "\t--> Scraped article 22 in category 'science'.\n",
      "\t--> Scraped article 23 in category 'science'.\n",
      "\t--> Scraped article 24 in category 'science'.\n",
      "\t--> Scraped article 25 in category 'science'.\n",
      "\t--> Scraped article 26 in category 'science'.\n",
      "\t--> Scraped article 27 in category 'science'.\n",
      "\t--> Scraped article 28 in category 'science'.\n",
      "\t--> Scraped article 29 in category 'science'.\n",
      "\t--> Scraped article 30 in category 'science'.\n",
      "\t--> Scraped article 31 in category 'science'.\n",
      "\t--> Scraped article 32 in category 'science'.\n",
      "\t--> Scraped article 33 in category 'science'.\n",
      "\t--> Scraped article 34 in category 'science'.\n",
      "\t--> Scraped article 35 in category 'science'.\n",
      "\t--> Scraped article 36 in category 'science'.\n",
      "\t--> Scraped article 37 in category 'science'.\n",
      "\t--> Scraped article 38 in category 'science'.\n",
      "\t--> Scraped article 39 in category 'science'.\n",
      "\t--> Scraped article 40 in category 'science'.\n",
      "\t--> Scraped article 41 in category 'science'.\n",
      "\t--> Scraped article 42 in category 'science'.\n",
      "\t--> Scraped article 43 in category 'science'.\n",
      "\t--> Scraped article 44 in category 'science'.\n",
      "\t--> Scraped article 45 in category 'science'.\n",
      "\t--> Scraped article 46 in category 'science'.\n",
      "\t--> Scraped article 47 in category 'science'.\n",
      "\t--> Scraped article 48 in category 'science'.\n",
      "\t--> Scraped article 49 in category 'science'.\n",
      "\t--> Scraped article 50 in category 'science'.\n",
      "\t--> Scraped article 51 in category 'science'.\n",
      "\t--> Scraped article 52 in category 'science'.\n",
      "\t--> Scraped article 53 in category 'science'.\n",
      "\t--> Scraped article 54 in category 'science'.\n",
      "\t--> Scraped article 55 in category 'science'.\n",
      "\t--> Scraped article 56 in category 'science'.\n",
      "\t--> Scraped article 57 in category 'science'.\n",
      "\t--> Scraped article 58 in category 'science'.\n",
      "\t--> Scraped article 59 in category 'science'.\n",
      "Completed scraping 59 articles from category 'science'.\n",
      "Scraping articles for category 'business'...\n",
      "\t--> Scraped article 1 in category 'business'.\n",
      "\t--> Scraped article 2 in category 'business'.\n",
      "\t--> Scraped article 3 in category 'business'.\n",
      "\t--> Scraped article 4 in category 'business'.\n",
      "\t--> Scraped article 5 in category 'business'.\n",
      "\t--> Scraped article 6 in category 'business'.\n",
      "\t--> Scraped article 7 in category 'business'.\n",
      "\t--> Scraped article 8 in category 'business'.\n",
      "\t--> Scraped article 9 in category 'business'.\n",
      "\t--> Scraped article 10 in category 'business'.\n",
      "\t--> Scraped article 11 in category 'business'.\n",
      "\t--> Scraped article 12 in category 'business'.\n",
      "\t--> Scraped article 13 in category 'business'.\n",
      "\t--> Scraped article 14 in category 'business'.\n",
      "\t--> Scraped article 15 in category 'business'.\n",
      "\t--> Scraped article 16 in category 'business'.\n",
      "\t--> Scraped article 17 in category 'business'.\n",
      "\t--> Scraped article 18 in category 'business'.\n",
      "\t--> Scraped article 19 in category 'business'.\n",
      "\t--> Scraped article 20 in category 'business'.\n",
      "\t--> Scraped article 21 in category 'business'.\n",
      "\t--> Scraped article 22 in category 'business'.\n",
      "\t--> Scraped article 23 in category 'business'.\n",
      "\t--> Scraped article 24 in category 'business'.\n",
      "\t--> Scraped article 25 in category 'business'.\n",
      "\t--> Scraped article 26 in category 'business'.\n",
      "\t--> Scraped article 27 in category 'business'.\n",
      "\t--> Scraped article 28 in category 'business'.\n",
      "\t--> Scraped article 29 in category 'business'.\n",
      "\t--> Scraped article 30 in category 'business'.\n",
      "\t--> Scraped article 31 in category 'business'.\n",
      "\t--> Scraped article 32 in category 'business'.\n",
      "\t--> Scraped article 33 in category 'business'.\n",
      "\t--> Scraped article 34 in category 'business'.\n",
      "\t--> Scraped article 35 in category 'business'.\n",
      "\t--> Scraped article 36 in category 'business'.\n",
      "\t--> Scraped article 37 in category 'business'.\n",
      "\t--> Scraped article 38 in category 'business'.\n",
      "\t--> Scraped article 39 in category 'business'.\n",
      "\t--> Scraped article 40 in category 'business'.\n",
      "\t--> Scraped article 41 in category 'business'.\n",
      "\t--> Scraped article 42 in category 'business'.\n",
      "\t--> Scraped article 43 in category 'business'.\n",
      "\t--> Scraped article 44 in category 'business'.\n",
      "\t--> Scraped article 45 in category 'business'.\n",
      "\t--> Scraped article 46 in category 'business'.\n",
      "\t--> Scraped article 47 in category 'business'.\n",
      "\t--> Scraped article 48 in category 'business'.\n",
      "\t--> Scraped article 49 in category 'business'.\n",
      "\t--> Scraped article 50 in category 'business'.\n",
      "\t--> Scraped article 51 in category 'business'.\n",
      "\t--> Scraped article 52 in category 'business'.\n",
      "\t--> Scraped article 53 in category 'business'.\n",
      "\t--> Scraped article 54 in category 'business'.\n",
      "\t--> Scraped article 55 in category 'business'.\n",
      "\t--> Scraped article 56 in category 'business'.\n",
      "Completed scraping 56 articles from category 'business'.\n",
      "Scraping articles for category 'world'...\n",
      "\t--> Scraped article 1 in category 'world'.\n",
      "\t--> Scraped article 2 in category 'world'.\n",
      "\t--> Scraped article 3 in category 'world'.\n",
      "\t--> Scraped article 4 in category 'world'.\n",
      "\t--> Scraped article 5 in category 'world'.\n",
      "\t--> Scraped article 6 in category 'world'.\n",
      "\t--> Scraped article 7 in category 'world'.\n",
      "\t--> Scraped article 8 in category 'world'.\n",
      "\t--> Scraped article 9 in category 'world'.\n",
      "\t--> Scraped article 10 in category 'world'.\n",
      "\t--> Scraped article 11 in category 'world'.\n",
      "\t--> Scraped article 12 in category 'world'.\n",
      "\t--> Scraped article 13 in category 'world'.\n",
      "\t--> Scraped article 14 in category 'world'.\n",
      "\t--> Scraped article 15 in category 'world'.\n",
      "\t--> Scraped article 16 in category 'world'.\n",
      "\t--> Scraped article 17 in category 'world'.\n",
      "\t--> Scraped article 18 in category 'world'.\n",
      "\t--> Scraped article 19 in category 'world'.\n",
      "\t--> Scraped article 20 in category 'world'.\n",
      "\t--> Scraped article 21 in category 'world'.\n",
      "\t--> Scraped article 22 in category 'world'.\n",
      "\t--> Scraped article 23 in category 'world'.\n",
      "\t--> Scraped article 24 in category 'world'.\n",
      "\t--> Scraped article 25 in category 'world'.\n",
      "\t--> Scraped article 26 in category 'world'.\n",
      "\t--> Scraped article 27 in category 'world'.\n",
      "\t--> Scraped article 28 in category 'world'.\n",
      "\t--> Scraped article 29 in category 'world'.\n",
      "\t--> Scraped article 30 in category 'world'.\n",
      "\t--> Scraped article 31 in category 'world'.\n",
      "\t--> Scraped article 32 in category 'world'.\n",
      "\t--> Scraped article 33 in category 'world'.\n",
      "\t--> Scraped article 34 in category 'world'.\n",
      "\t--> Scraped article 35 in category 'world'.\n",
      "\t--> Scraped article 36 in category 'world'.\n",
      "\t--> Scraped article 37 in category 'world'.\n",
      "\t--> Scraped article 38 in category 'world'.\n",
      "\t--> Scraped article 39 in category 'world'.\n",
      "\t--> Scraped article 40 in category 'world'.\n",
      "\t--> Scraped article 41 in category 'world'.\n",
      "\t--> Scraped article 42 in category 'world'.\n",
      "\t--> Scraped article 43 in category 'world'.\n",
      "\t--> Scraped article 44 in category 'world'.\n",
      "\t--> Scraped article 45 in category 'world'.\n",
      "\t--> Scraped article 46 in category 'world'.\n",
      "\t--> Scraped article 47 in category 'world'.\n",
      "\t--> Scraped article 48 in category 'world'.\n",
      "\t--> Scraped article 49 in category 'world'.\n",
      "\t--> Scraped article 50 in category 'world'.\n",
      "\t--> Scraped article 51 in category 'world'.\n",
      "\t--> Scraped article 52 in category 'world'.\n",
      "\t--> Scraped article 53 in category 'world'.\n",
      "\t--> Scraped article 54 in category 'world'.\n",
      "\t--> Scraped article 55 in category 'world'.\n",
      "\t--> Scraped article 56 in category 'world'.\n",
      "Completed scraping 56 articles from category 'world'.\n",
      "Scraping articles for category 'entertainment'...\n",
      "\t--> Scraped article 1 in category 'entertainment'.\n",
      "\t--> Scraped article 2 in category 'entertainment'.\n",
      "\t--> Scraped article 3 in category 'entertainment'.\n",
      "\t--> Scraped article 4 in category 'entertainment'.\n",
      "\t--> Scraped article 5 in category 'entertainment'.\n",
      "\t--> Scraped article 6 in category 'entertainment'.\n",
      "\t--> Scraped article 7 in category 'entertainment'.\n",
      "\t--> Scraped article 8 in category 'entertainment'.\n",
      "\t--> Scraped article 9 in category 'entertainment'.\n",
      "\t--> Scraped article 10 in category 'entertainment'.\n",
      "\t--> Scraped article 11 in category 'entertainment'.\n",
      "\t--> Scraped article 12 in category 'entertainment'.\n",
      "\t--> Scraped article 13 in category 'entertainment'.\n",
      "\t--> Scraped article 14 in category 'entertainment'.\n",
      "\t--> Scraped article 15 in category 'entertainment'.\n",
      "\t--> Scraped article 16 in category 'entertainment'.\n",
      "\t--> Scraped article 17 in category 'entertainment'.\n",
      "\t--> Scraped article 18 in category 'entertainment'.\n",
      "\t--> Scraped article 19 in category 'entertainment'.\n",
      "\t--> Scraped article 20 in category 'entertainment'.\n",
      "\t--> Scraped article 21 in category 'entertainment'.\n",
      "\t--> Scraped article 22 in category 'entertainment'.\n",
      "\t--> Scraped article 23 in category 'entertainment'.\n",
      "\t--> Scraped article 24 in category 'entertainment'.\n",
      "\t--> Scraped article 25 in category 'entertainment'.\n",
      "\t--> Scraped article 26 in category 'entertainment'.\n",
      "\t--> Scraped article 27 in category 'entertainment'.\n",
      "\t--> Scraped article 28 in category 'entertainment'.\n",
      "\t--> Scraped article 29 in category 'entertainment'.\n",
      "\t--> Scraped article 30 in category 'entertainment'.\n",
      "\t--> Scraped article 31 in category 'entertainment'.\n",
      "\t--> Scraped article 32 in category 'entertainment'.\n",
      "\t--> Scraped article 33 in category 'entertainment'.\n",
      "\t--> Scraped article 34 in category 'entertainment'.\n",
      "\t--> Scraped article 35 in category 'entertainment'.\n",
      "\t--> Scraped article 36 in category 'entertainment'.\n",
      "\t--> Scraped article 37 in category 'entertainment'.\n",
      "\t--> Scraped article 38 in category 'entertainment'.\n",
      "\t--> Scraped article 39 in category 'entertainment'.\n",
      "\t--> Scraped article 40 in category 'entertainment'.\n",
      "\t--> Scraped article 41 in category 'entertainment'.\n",
      "\t--> Scraped article 42 in category 'entertainment'.\n",
      "\t--> Scraped article 43 in category 'entertainment'.\n",
      "\t--> Scraped article 44 in category 'entertainment'.\n",
      "\t--> Scraped article 45 in category 'entertainment'.\n",
      "\t--> Scraped article 46 in category 'entertainment'.\n",
      "\t--> Scraped article 47 in category 'entertainment'.\n",
      "\t--> Scraped article 48 in category 'entertainment'.\n",
      "\t--> Scraped article 49 in category 'entertainment'.\n",
      "\t--> Scraped article 50 in category 'entertainment'.\n",
      "\t--> Scraped article 51 in category 'entertainment'.\n",
      "\t--> Scraped article 52 in category 'entertainment'.\n",
      "\t--> Scraped article 53 in category 'entertainment'.\n",
      "\t--> Scraped article 54 in category 'entertainment'.\n",
      "\t--> Scraped article 55 in category 'entertainment'.\n",
      "\t--> Scraped article 56 in category 'entertainment'.\n",
      "\t--> Scraped article 57 in category 'entertainment'.\n",
      "\t--> Scraped article 58 in category 'entertainment'.\n",
      "\t--> Scraped article 59 in category 'entertainment'.\n",
      "Completed scraping 59 articles from category 'entertainment'.\n",
      "Articles saved to geo_articles.csv\n"
     ]
    }
   ],
   "source": [
    "class Geo_Scraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "    def get_geo_articles(self, max_articles_per_category=100):\n",
    "        geo_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        categories = {\n",
    "            \"sports\": \"https://www.geo.tv/category/sports\",\n",
    "            \"science\": \"https://www.geo.tv/category/sci-tech\",\n",
    "            \"business\": \"https://www.geo.tv/category/business\",\n",
    "            \"world\": \"https://www.geo.tv/category/world\",\n",
    "            \"entertainment\": \"https://www.geo.tv/category/entertainment\"\n",
    "        }\n",
    "        for category, url in categories.items():\n",
    "            article_count = 0\n",
    "            print(f\"Scraping articles for category '{category}'...\")\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            articles = soup.find_all(\"li\", class_=\"border-box\")\n",
    "            if not articles:\n",
    "                print(f\"No articles found in category '{category}'\")\n",
    "                continue\n",
    "            for article in articles:\n",
    "                title_tag = article.find(\"a\", class_=\"open-section\")\n",
    "                title = title_tag.get(\"title\", \"Title not found\")\n",
    "                link = title_tag[\"href\"]\n",
    "                if link.endswith('-'):\n",
    "                    article_id = link.split('-')[-1] \n",
    "                    if len(article_id) <= 3:\n",
    "                        next_two_digits = article_id[2:]  \n",
    "                        link = link + next_two_digits\n",
    "                    else:\n",
    "                        link = link + article_id[:2]  \n",
    "                article_response = requests.get(link)\n",
    "                article_response.raise_for_status()\n",
    "                article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                content_div = article_soup.find(\"div\", class_=\"content-area\")\n",
    "                paragraphs = content_div.find_all(\"p\") if content_div else []\n",
    "                content = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "                geo_df[\"id\"].append(self.id)\n",
    "                geo_df[\"title\"].append(title)\n",
    "                geo_df[\"link\"].append(link)\n",
    "                geo_df[\"content\"].append(content)\n",
    "                geo_df[\"gold_label\"].append(category)\n",
    "                self.id += 1\n",
    "                article_count += 1\n",
    "                print(f\"\\t--> Scraped article {article_count} in category '{category}'.\")\n",
    "                if article_count >= max_articles_per_category:\n",
    "                    break\n",
    "            print(f\"Completed scraping {article_count} articles from category '{category}'.\")\n",
    "        df = pd.DataFrame(geo_df)\n",
    "        return df\n",
    "    \n",
    "    def save_to_csv(self, df, filename=\"geo_articles.csv\"):\n",
    "        with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "            writer.writerow([\"ID\", \"Title\", \"Link\", \"Content\", \"Gold Label\"])\n",
    "            for _, row in df.iterrows():\n",
    "                writer.writerow([row[\"id\"], row[\"title\"], row[\"link\"], row[\"content\"], row[\"gold_label\"]])\n",
    "        print(f\"Articles saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Samaa TV: <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping category: Business\n",
      "Fetching page: https://www.samaa.tv/money?page=1\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/money?page=2\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/money?page=3\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/money?page=4\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/money?page=5\n",
      "response:  <Response [200]>\n",
      "Found 200 articles in Business.\n",
      "Scraping category: Science-Technology\n",
      "Fetching page: https://www.samaa.tv/tech?page=1\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/tech?page=2\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/tech?page=3\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/tech?page=4\n",
      "response:  <Response [200]>\n",
      "Found 200 articles in Science-Technology.\n",
      "Scraping category: International\n",
      "Fetching page: https://www.samaa.tv/global?page=1\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/global?page=2\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/global?page=3\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/global?page=4\n",
      "response:  <Response [200]>\n",
      "Found 200 articles in International.\n",
      "Scraping category: Sports\n",
      "Fetching page: https://www.samaa.tv/sports?page=1\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/sports?page=2\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/sports?page=3\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/sports?page=4\n",
      "response:  <Response [200]>\n",
      "Found 200 articles in Sports.\n",
      "Scraping category: Entertainment\n",
      "Fetching page: https://www.samaa.tv/lifestyle?page=1\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/lifestyle?page=2\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/lifestyle?page=3\n",
      "response:  <Response [200]>\n",
      "Fetching page: https://www.samaa.tv/lifestyle?page=4\n",
      "response:  <Response [200]>\n",
      "Found 200 articles in Entertainment.\n",
      "Articles saved to samaa_articles.csv\n"
     ]
    }
   ],
   "source": [
    "class SamaaScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.samaa.tv/\"  \n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
    "        }\n",
    "        self.articles = []\n",
    "\n",
    "    def fetch_category_articles(self, category_url, category_name):\n",
    "        \"\"\"\n",
    "        Fetches up to 80 articles from a category page with pagination.\n",
    "        \"\"\"\n",
    "        page = 1\n",
    "        while len([a for a in self.articles if a['category'] == category_name]) < 200:\n",
    "            url = f\"{category_url}?page={page}\"\n",
    "            print(f\"Fetching page: {url}\")\n",
    "            response = requests.get(url, headers=self.headers)  \n",
    "            \n",
    "            print(\"response: \", response)\n",
    "            if response.status_code == 403:\n",
    "                print(f\"Access forbidden (403) for {url}. Check headers or other access restrictions.\")\n",
    "                break\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"Error fetching page {page} of {category_name}. Status code: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            articles_in_category = []\n",
    "            for article in soup.select('article.story-article'):\n",
    "                title = article.h3.a.text.strip()\n",
    "                link = article.h3.a['href']\n",
    "                if not link.startswith(\"http\"):\n",
    "                    link = self.base_url + link\n",
    "                full_content = self.fetch_article_content(link)\n",
    "                articles_in_category.append({\n",
    "                    'title': title,\n",
    "                    'link': link,\n",
    "                    'content': full_content,\n",
    "                    'category': category_name,\n",
    "                    'id': len(self.articles) + len(articles_in_category)\n",
    "                })\n",
    "                if len(articles_in_category) + len([a for a in self.articles if a['category'] == category_name]) >= 200:\n",
    "                    break\n",
    "            \n",
    "            if not articles_in_category:\n",
    "                break \n",
    "            \n",
    "            self.articles.extend(articles_in_category)\n",
    "            page += 1\n",
    "\n",
    "    def fetch_article_content(self, url):\n",
    "        \"\"\"\n",
    "        Fetches the full content of an article from its URL.\n",
    "        \"\"\"\n",
    "        response = requests.get(url, headers=self.headers)  \n",
    "        article_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        content_div = article_soup.find('div', class_='article-content')\n",
    "        \n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            full_content = ' '.join(paragraph.text.strip() for paragraph in paragraphs)\n",
    "            return full_content\n",
    "        else:\n",
    "            print(f\"Warning: No content found for URL {url}\")\n",
    "            return \"\"\n",
    "\n",
    "    def scrape(self, categories):\n",
    "        \"\"\"\n",
    "        Main scraping function to fetch articles from multiple categories.\n",
    "        \"\"\"\n",
    "        for category_name, category_url in categories.items():\n",
    "            print(f\"Scraping category: {category_name}\")\n",
    "            self.fetch_category_articles(category_url, category_name)\n",
    "            print(f\"Found {len([a for a in self.articles if a['category'] == category_name])} articles in {category_name}.\")\n",
    "\n",
    "    def save_to_csv(self, filename=\"articles.csv\"):\n",
    "        \"\"\"\n",
    "        Saves the scraped articles to a CSV file with quotes around text fields.\n",
    "        \"\"\"\n",
    "        with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file, quoting=csv.QUOTE_ALL)  \n",
    "            writer.writerow([\"ID\", \"Category\", \"Title\", \"Link\", \"Content\"])\n",
    "            for article in self.articles:\n",
    "                writer.writerow([article['id'], article['category'], article['title'], article['link'], article['content']])\n",
    "        print(f\"Articles saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Store data scraped from the scrapers into csv files: <h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_scraper = Geo_Scraper()\n",
    "df = geo_scraper.get_geo_articles(max_articles_per_category=60)\n",
    "geo_scraper.save_to_csv(df, filename=\"geo_articles.csv\")\n",
    "\n",
    "\n",
    "categories = {\n",
    "    \"business\": \"https://urdu.samaa.tv/money\",\n",
    "    \"science\": \"https://urdu.samaa.tv/tech\",\n",
    "    \"world\": \"https://urdu.samaa.tv/global\",\n",
    "    \"sports\": \"https://urdu.samaa.tv/sports\",\n",
    "    \"entertainment\": \"https://urdu.samaa.tv/lifestyle\",\n",
    "}\n",
    "\n",
    "scraper = SamaaScraper()\n",
    "scraper.scrape(categories)\n",
    "scraper.save_to_csv(\"samaa_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Generate a combined CSV file: <h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as 'combined_articles.csv'\n"
     ]
    }
   ],
   "source": [
    "geo_df = pd.read_csv(\"geo_articles.csv\")\n",
    "samaa_df = pd.read_csv(\"samaa_articles.csv\")\n",
    "\n",
    "samaa_df = samaa_df.rename(columns={\"Category\": \"Gold Label\"})\n",
    "\n",
    "desired_columns = [\"ID\", \"Title\", \"Link\", \"Content\", \"Gold Label\"]\n",
    "geo_df = geo_df[desired_columns]\n",
    "samaa_df = samaa_df[desired_columns]\n",
    "\n",
    "merged_df = pd.concat([geo_df, samaa_df], ignore_index=True)\n",
    "\n",
    "merged_df[\"ID\"] = range(len(merged_df))\n",
    "\n",
    "merged_df.to_csv(\"combined_articles.csv\", index=False)\n",
    "\n",
    "print(\"Merged file saved as 'combined_articles.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
